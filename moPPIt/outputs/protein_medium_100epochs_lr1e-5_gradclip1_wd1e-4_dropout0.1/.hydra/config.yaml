mode: ppl_eval
diffusion: uniform
backbone: dit
classifier_backbone: null
parameterization: d3pm
time_conditioning: true
subs_masking: false
zero_recon_loss: true
T: 0
is_vision: false
seed: 42
loader:
  global_batch_size: 512
  eval_global_batch_size: ${.global_batch_size}
  batch_size: ${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}
  eval_batch_size: ${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices}
    * ${trainer.num_nodes}}}
  num_workers: 0
  pin_memory: true
  persistent_workers: false
sampling:
  use_cache: true
  steps: 32
  batch_size: 1
  num_sample_batches: 500
  use_float64: false
eval:
  checkpoint_path: /home/tc415/muPPIt_embedding/muppit/model_path/PeptideUDLM.ckpt
  target_sequence: QAPEFLLGDGSFGSVYRAAYEGEEVAVKIFNKHTSLRLLRQELVVLCHLHHPSLISLLAAGIRPRMLVMELASKGSLDRLLQQDKASLTRTLQHRIALHVADGLRYLHSAMIIYRDLKPHNVLLFTLYPNAAIIAKIADYSIAQYCCRMGIKTSEGTPGFRAPEVARGNVIYNQQADVYSFGLLLYDILTTGGRIVEGLKFPNEFDELEIQGKLPDPVKEYGCAPWPMVEKLIKQCLKENPQERPTSAQVFDILNSAELV
  target_motifs: 139-141
  disable_ema: false
  generate_samples: true
  generated_samples_path: ''
  max_samples: 50000
training:
  ema: 0.9999
  antithetic_sampling: true
  importance_sampling: false
  sampling_eps: 0.001
  change_of_variables: false
  compute_loss_on_pad_tokens: true
  use_simple_ce_loss: false
  guidance: null
optim:
  weight_decay: 0.0001
  lr: 1.0e-05
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-08
trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  devices: 2
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  precision: bf16-mixed
  num_sanity_val_steps: 2
  max_steps: 1652000
  log_every_n_steps: 100
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  val_check_interval: 16520
wandb:
  project: moPPIt-v2
  job_type: model-training
  name: protein_medium_100epochs_lr1e-5_gradclip1_wd1e-4_dropout0.1
  id: ${.name}
checkpointing:
  save_dir: ${cwd:}
  resume_from_ckpt: false
  resume_ckpt_path: ${.save_dir}/checkpoints/last.ckpt
callbacks:
  checkpoint_every_n_steps:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: -1
    save_last: true
    dirpath: ${checkpointing.save_dir}/checkpoints
    verbose: true
    auto_insert_metric_name: false
  checkpoint_monitor:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/nll
    mode: min
    save_top_k: 1
    save_last: false
    dirpath: ${checkpointing.save_dir}/checkpoints
    filename: best
    auto_insert_metric_name: false
    verbose: true
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
data:
  train: peptide
  valid: peptide
  tokenizer_name_or_path: facebook/esm2_t33_650M_UR50D
  cache_dir: /home/tc415/discrete-diffusion-guidance/dataset
  wrap: false
  streaming: false
  override_cache: false
  add_special_tokens: true
model:
  name: small
  type: ddit
  hidden_size: 768
  cond_dim: 128
  length: 11
  n_blocks: 12
  n_heads: 12
  scale_by_sigma: true
  dropout: 0.1
  tie_word_embeddings: false
strategy:
  _target_: lightning.pytorch.strategies.DDPStrategy
  find_unused_parameters: false
noise:
  type: loglinear
  sigma_min: 0.0001
  sigma_max: 20
lr_scheduler:
  _target_: utils.CosineDecayWarmupLRScheduler
  t_in_epochs: false
  t_initial: ${eval:${trainer.max_steps}-${.warmup_t}}
  warmup_prefix: true
  warmup_lr_init: 1.0e-07
  warmup_t: ${eval:0.1*${trainer.max_steps}}
  lr_min: 1.0e-07
guidance:
  method: cbg
  condition: 0
  classifier_checkpoint_path: /home/tc415/muPPIt_embedding/muppit/model_path/PeptideBindEvaluator.ckpt
  gamma: 2.0
  use_approx: false
